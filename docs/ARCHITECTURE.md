# Data Architecture Recommendation - First-Time User Optimized

**Date**: November 20, 2025
**Purpose**: Design optimal data collection and storage for **first-time user experience**
**Key Principle**: All dashboards work perfectly on first login (except Deep User Analytics)

---

## Executive Summary

**Product Vision**: Build a powerful analytics dashboard that provides instant insights on first login, with optional historical tracking for power users.

**Architecture**: **Single comprehensive snapshot** (current state) + **historical archive** (Deep User only)

**User Experience**:
- âœ… **First-time user**: Sync once (90 seconds) â†’ All dashboards fully functional
- âœ… **Returning user**: Auto-refresh if >24hrs, all dashboards update
- âœ… **Deep User page**: Special page showing trends over time (requires multiple visits)

---

## Table of Contents
1. [Product Philosophy](#product-philosophy)
2. [Simplified File Structure](#simplified-file-structure)
3. [Initial Data Sync (First-Time User)](#initial-data-sync-first-time-user)
4. [Dashboard Data Requirements](#dashboard-data-requirements)
5. [Deep User Analytics (Exception)](#deep-user-analytics-exception)
6. [Implementation Guide](#implementation-guide)
7. [Rate Limiting & Performance](#rate-limiting--performance)

---

## Product Philosophy

### Design Principle: First-Time User First

**Primary Use Case**: User connects Spotify once, gets instant comprehensive insights
**Secondary Use Case**: Power users who return see trends over time (Deep User page)

**Why This Matters**:
- Most users will only connect once (curiosity, one-time analysis)
- Must provide value immediately, not "come back next week"
- Spotify provides rich data RIGHT NOW (top tracks all-time, 6 months, 4 weeks)
- Historical tracking is a bonus feature, not a requirement

### Dashboard Tiers

**Tier 1: Instant Insights (First-Time User Friendly)**
All these pages work perfectly with just the initial sync:
- **Dashboard** - Overview metrics, top artists, temporal patterns
- **Advanced Analytics** - Audio features, mood analysis, distributions
- **Recent Listening** - Last 50 tracks with timestamps
- **Top Tracks** - Top 50 across 3 time ranges
- **Playlists** - User's playlists overview (future feature)

**Tier 2: Historical Insights (Multi-Visit Feature)**
- **Deep User Analytics** - Trends over time, artist evolution, listening pattern changes
  - First-time users: Show single data point with note "Come back to see trends!"
  - Returning users: Show time-series comparisons

---

## Simplified File Structure

### Cloudflare R2 Layout

```
cloudflare-r2://ime565spotify/
â”‚
â”œâ”€â”€ reference_data/
â”‚   â””â”€â”€ kaggle_tracks_audio_features.parquet    # Audio features lookup (shared)
â”‚
â””â”€â”€ users/
    â””â”€â”€ {user_id}/                              # e.g., "nico_diferd"
        â”‚
        â”œâ”€â”€ current/                            # ðŸ”¥ USED BY ALL DASHBOARDS
        â”‚   â”œâ”€â”€ metadata.json                   # Last sync time, user profile
        â”‚   â”œâ”€â”€ recent_tracks.parquet           # 50 recent tracks
        â”‚   â”œâ”€â”€ top_tracks_short.parquet        # Top 50 (last 4 weeks)
        â”‚   â”œâ”€â”€ top_tracks_medium.parquet       # Top 50 (last 6 months)
        â”‚   â”œâ”€â”€ top_tracks_long.parquet         # Top 50 (all-time)
        â”‚   â”œâ”€â”€ top_artists_short.parquet       # Top 50 artists (4 weeks)
        â”‚   â”œâ”€â”€ top_artists_medium.parquet      # Top 50 artists (6 months)
        â”‚   â”œâ”€â”€ top_artists_long.parquet        # Top 50 artists (all-time)
        â”‚   â””â”€â”€ computed_metrics.json           # Derived metrics
        â”‚
        â””â”€â”€ snapshots/                          # ðŸ“Š ONLY FOR DEEP USER PAGE
            â”œâ”€â”€ 2025-11-20T14-30-00Z/
            â”‚   â””â”€â”€ [same files as current/]
            â”œâ”€â”€ 2025-11-21T10-15-00Z/
            â”‚   â””â”€â”€ [same files as current/]
            â””â”€â”€ ...
```

### Key Design Decisions

**1. `current/` Directory**
- **Purpose**: Latest snapshot, used by ALL dashboards (except Deep User)
- **Update Strategy**: Overwrite on every sync (no append)
- **Loading**: Fast, single directory read
- **Caching**: Streamlit `@st.cache_data(ttl=3600)` for instant dashboard loads

**2. `snapshots/` Directory**
- **Purpose**: Historical archive for Deep User page ONLY
- **Update Strategy**: Append-only (never delete)
- **Loading**: Lazy load only when Deep User page is accessed
- **First-time users**: Directory is empty or has 1 snapshot

**3. Why This Structure?**
- âœ… **Instant value**: All dashboards read from `current/` on first visit
- âœ… **Simple caching**: One directory to cache, predictable paths
- âœ… **Fast loading**: No need to scan/aggregate multiple snapshots
- âœ… **Optional history**: Deep User page loads `snapshots/` on-demand
- âœ… **Low complexity**: Two simple directories, clear separation of concerns

---

## Initial Data Sync (First-Time User)

### What Gets Collected (8 API Calls)

```python
def collect_initial_snapshot(sp, user_id):
    """
    Initial comprehensive sync for first-time user
    Target: <90 seconds total duration
    Rate limit: 8 API calls (well within 180/min limit)
    """

    # 1. User Profile (1 call)
    profile = sp.current_user()

    # 2. Recently Played (1 call)
    recent_tracks = sp.current_user_recently_played(limit=50)

    # 3. Top Tracks - 3 Time Ranges (3 calls)
    top_tracks_short = sp.current_user_top_tracks(time_range='short_term', limit=50)
    top_tracks_medium = sp.current_user_top_tracks(time_range='medium_term', limit=50)
    top_tracks_long = sp.current_user_top_tracks(time_range='long_term', limit=50)

    # 4. Top Artists - 3 Time Ranges (3 calls)
    top_artists_short = sp.current_user_top_artists(time_range='short_term', limit=50)
    top_artists_medium = sp.current_user_top_artists(time_range='medium_term', limit=50)
    top_artists_long = sp.current_user_top_artists(time_range='long_term', limit=50)

    # Total: 8 API calls
    # Estimated time: 10-15 seconds for API calls + 5-10 seconds processing
    # Well within 90 second target
```

### What About Playlists?

**Decision**: Skip for now (future feature)

**Reasoning**:
- Can be 50-200+ playlists per user
- Each playlist requires additional calls to get tracks
- Could easily exceed 90 second target
- Playlists page is lower priority than core analytics

**Future**: Add as separate optional sync ("Analyze My Playlists")

---

## Dashboard Data Requirements

### Dashboard 1: Main Dashboard

**Purpose**: Overview of listening habits
**Data Source**: `current/` directory ONLY

**Metrics Available**:
- Top artists (short/medium/long term comparison)
- Artist diversity score
- Genre distribution (from artist data)
- Temporal patterns (hour-of-day, day-of-week from recent_tracks)
- Listening streaks (consecutive days from recent_tracks)
- Average popularity (mainstream vs niche)

**Why It Works on First Visit**:
- Spotify provides top artists across 3 time ranges
- Can compare "last 4 weeks" vs "all-time" to show taste evolution
- Recent 50 tracks give temporal patterns
- No historical snapshots needed

**Example Insights**:
> "Your top artist this month is Drake, but all-time it's The Weeknd. You're a Night Owl - 60% of listening happens after 8pm."

---

### Dashboard 2: Advanced Analytics

**Purpose**: Deep dive into audio features and mood
**Data Source**: `current/` directory + Kaggle lookup

**Metrics Available**:
- Audio feature distributions (danceability, energy, valence)
- Mood scores (composite metrics)
- Acoustic vs Electronic preference
- Vocal vs Instrumental preference
- Context classification (workout, focus, party, relaxation)

**Why It Works on First Visit**:
- Enrich current top tracks with Kaggle audio features
- Compute distributions and averages
- Show audio feature radar chart
- Compare short-term vs long-term audio profiles

**Audio Features Strategy**:
1. Load `current/top_tracks_*.parquet`
2. Merge with `reference_data/kaggle_tracks_audio_features.parquet` on track_id
3. Show coverage: "Audio features available for 67% of your top tracks"
4. Display analytics for matched tracks

**Example Insights**:
> "Your music is High Energy (0.78) and Happy (valence 0.65). 45% of your top tracks are optimal for focus (low speechiness, high instrumentalness)."

---

### Dashboard 3: Recent Listening

**Purpose**: Timeline of recently played tracks
**Data Source**: `current/recent_tracks.parquet`

**Metrics Available**:
- Last 50 tracks with timestamps
- Hour-of-day heatmap
- Day-of-week patterns
- Average track duration
- Explicit content ratio
- Release year distribution

**Why It Works on First Visit**:
- Recent 50 tracks include `played_at` timestamps
- Can analyze temporal patterns within this window
- Enough data for meaningful hour/day patterns

**Example Insights**:
> "You listened to 50 tracks over the last 3 days. Peak listening: Tuesday 9-11am and Friday 6-10pm. 40% explicit content."

---

### Dashboard 4: Top Tracks

**Purpose**: Your favorite songs across different time periods
**Data Source**: `current/top_tracks_*.parquet`

**Metrics Available**:
- Top 50 tracks for each time range (short/medium/long)
- Overlap analysis (how many short-term tracks are in all-time?)
- Audio feature profiles per time range
- Taste consistency score
- Discovery vs loyalty ratio

**Why It Works on First Visit**:
- Spotify API provides top tracks for 3 time ranges
- Can compare current favorites vs all-time favorites
- Shows taste evolution within single session

**Example Insights**:
> "Musical Explorer: Only 20% of your current top tracks are in your all-time favorites. You're constantly discovering new music."
>
> "Musical Consistency: 70% of your current favorites are also all-time favorites. You know what you like!"

---

### Dashboard 5: Playlists (Future)

**Purpose**: Playlist health and optimization
**Data Source**: User playlists (not in initial sync)

**Status**: Deferred (not in 90-second sync)

---

### Dashboard 6: Deep User Analytics (EXCEPTION)

**Purpose**: Show trends over time - the ONLY page that needs historical data
**Data Source**: `snapshots/` directory

**First-Time User Experience**:
- Show current snapshot as a single data point
- Display message: "Deep User Analytics tracks your listening evolution over time. Come back in a week to see how your taste changes!"
- Show placeholder charts with single point
- Explain what will appear with more visits

**Returning User Experience (2+ snapshots)**:
- Artist evolution (top artists month-over-month)
- Listening pattern changes (temporal shifts)
- Mainstream score trajectory
- Genre drift analysis
- Discovery rate over time

**Metrics Available** (with multiple snapshots):
- Top artist changes month-over-month
- Genre distribution trends
- Listening intensity changes (tracks per week)
- Mainstream score over time
- Artist diversity trends

**Example Insights** (multi-visit):
> "Your taste is becoming more niche: Mainstream score dropped from 68 to 52 over 6 months. You discovered 12 new artists in November, up from 5 in October."

**First-Time User Message**:
```
ðŸ“Š Deep User Analytics

This page shows how your listening habits evolve over time.

Current Status: First visit (1 snapshot collected)
Next Update: Return in 24 hours to see trends

What you'll see with more data:
- How your top artists change each week
- Listening pattern shifts (weekday vs weekend)
- Taste evolution (mainstream â†’ niche or vice versa)
- Discovery trends (new artists per month)

[View Current Snapshot] [Set Reminder]
```

---

## Implementation Guide

### Step 1: Refactor `data_collection.py`

```python
# app/func/data_collection.py

import pandas as pd
from datetime import datetime, timezone
from .s3_storage import save_to_r2, load_from_r2, save_parquet_to_r2, load_parquet_from_r2
import time

def should_refresh_data(user_id):
    """
    Check if we need to refresh current snapshot
    Returns True if >24 hours since last sync OR no data exists
    """
    try:
        metadata = load_from_r2(f'users/{user_id}/current/metadata.json')
        last_sync = datetime.fromisoformat(metadata['last_sync'])
        hours_since = (datetime.now(timezone.utc) - last_sync).total_seconds() / 3600
        return hours_since >= 24
    except:
        return True  # No data yet, refresh needed


def collect_comprehensive_snapshot(sp, user_id, force=False):
    """
    Collect comprehensive snapshot for all dashboards

    Target: <90 seconds total
    API Calls: 8 total (well within rate limits)

    Args:
        sp: Authenticated Spotipy client
        user_id: Spotify user ID
        force: Force refresh even if <24hrs

    Returns:
        success: bool
        snapshot_timestamp: str
    """

    if not force and not should_refresh_data(user_id):
        return False, None  # Skip refresh

    print(f"ðŸ”„ Starting comprehensive data sync for {user_id}")
    start_time = time.time()

    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H-%M-%S')

    try:
        # ========================================
        # Step 1: User Profile (1 API call)
        # ========================================
        print("  ðŸ“‹ Fetching user profile...")
        profile = sp.current_user()

        metadata = {
            'last_sync': datetime.now(timezone.utc).isoformat(),
            'snapshot_timestamp': timestamp,
            'user_id': user_id,
            'display_name': profile.get('display_name'),
            'country': profile.get('country'),
            'product': profile.get('product'),
            'api_version': '2025-11'
        }

        # ========================================
        # Step 2: Recently Played (1 API call)
        # ========================================
        print("  ðŸŽµ Fetching recently played tracks...")
        recent = sp.current_user_recently_played(limit=50)
        recent_df = process_recent_tracks(recent)

        # ========================================
        # Step 3: Top Tracks - All Time Ranges (3 API calls)
        # ========================================
        print("  ðŸ† Fetching top tracks (short-term)...")
        top_tracks_short = sp.current_user_top_tracks(time_range='short_term', limit=50)
        tracks_short_df = process_top_tracks(top_tracks_short, 'short_term')

        print("  ðŸ† Fetching top tracks (medium-term)...")
        top_tracks_medium = sp.current_user_top_tracks(time_range='medium_term', limit=50)
        tracks_medium_df = process_top_tracks(top_tracks_medium, 'medium_term')

        print("  ðŸ† Fetching top tracks (long-term)...")
        top_tracks_long = sp.current_user_top_tracks(time_range='long_term', limit=50)
        tracks_long_df = process_top_tracks(top_tracks_long, 'long_term')

        # ========================================
        # Step 4: Top Artists - All Time Ranges (3 API calls)
        # ========================================
        print("  ðŸ‘¥ Fetching top artists (short-term)...")
        top_artists_short = sp.current_user_top_artists(time_range='short_term', limit=50)
        artists_short_df = process_top_artists(top_artists_short, 'short_term')

        print("  ðŸ‘¥ Fetching top artists (medium-term)...")
        top_artists_medium = sp.current_user_top_artists(time_range='medium_term', limit=50)
        artists_medium_df = process_top_artists(top_artists_medium, 'medium_term')

        print("  ðŸ‘¥ Fetching top artists (long-term)...")
        top_artists_long = sp.current_user_top_artists(time_range='long_term', limit=50)
        artists_long_df = process_top_artists(top_artists_long, 'long_term')

        # ========================================
        # Step 5: Compute Derived Metrics
        # ========================================
        print("  ðŸ“Š Computing metrics...")
        metrics = compute_snapshot_metrics(
            recent_df,
            tracks_short_df, tracks_medium_df, tracks_long_df,
            artists_short_df, artists_medium_df, artists_long_df
        )

        # ========================================
        # Step 6: Save to R2 (current/ directory)
        # ========================================
        print("  ðŸ’¾ Saving to cloud storage...")

        # Save to current/ (overwrites previous)
        save_to_r2(f'users/{user_id}/current/metadata.json', metadata)
        save_parquet_to_r2(f'users/{user_id}/current/recent_tracks.parquet', recent_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_tracks_short.parquet', tracks_short_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_tracks_medium.parquet', tracks_medium_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_tracks_long.parquet', tracks_long_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_artists_short.parquet', artists_short_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_artists_medium.parquet', artists_medium_df)
        save_parquet_to_r2(f'users/{user_id}/current/top_artists_long.parquet', artists_long_df)
        save_to_r2(f'users/{user_id}/current/computed_metrics.json', metrics)

        # ========================================
        # Step 7: Archive to snapshots/ (for Deep User page)
        # ========================================
        print("  ðŸ“¦ Archiving snapshot for historical analysis...")

        snapshot_dir = f'users/{user_id}/snapshots/{timestamp}'
        save_to_r2(f'{snapshot_dir}/metadata.json', metadata)
        save_parquet_to_r2(f'{snapshot_dir}/recent_tracks.parquet', recent_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_tracks_short.parquet', tracks_short_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_tracks_medium.parquet', tracks_medium_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_tracks_long.parquet', tracks_long_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_artists_short.parquet', artists_short_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_artists_medium.parquet', artists_medium_df)
        save_parquet_to_r2(f'{snapshot_dir}/top_artists_long.parquet', artists_long_df)
        save_to_r2(f'{snapshot_dir}/computed_metrics.json', metrics)

        elapsed = time.time() - start_time
        print(f"  âœ… Sync complete in {elapsed:.1f} seconds")

        return True, timestamp

    except Exception as e:
        print(f"  âŒ Sync failed: {e}")
        return False, None


def process_recent_tracks(recent_response):
    """Convert recent tracks API response to DataFrame"""
    tracks = []
    for item in recent_response['items']:
        track = item['track']
        tracks.append({
            'played_at': item['played_at'],
            'track_id': track['id'],
            'track_name': track['name'],
            'artist_id': track['artists'][0]['id'],
            'artist_name': track['artists'][0]['name'],
            'album_name': track['album']['name'],
            'album_type': track['album']['album_type'],
            'release_date': track['album']['release_date'],
            'popularity': track['popularity'],
            'duration_ms': track['duration_ms'],
            'explicit': track['explicit'],
        })

    df = pd.DataFrame(tracks)

    # Add derived columns
    df['played_at_dt'] = pd.to_datetime(df['played_at'])
    df['hour_of_day'] = df['played_at_dt'].dt.hour
    df['day_of_week'] = df['played_at_dt'].dt.dayofweek
    df['day_name'] = df['played_at_dt'].dt.day_name()
    df['duration_seconds'] = df['duration_ms'] / 1000
    df['release_year'] = df['release_date'].str[:4].astype(int)

    return df


def process_top_tracks(top_tracks_response, time_range):
    """Convert top tracks API response to DataFrame"""
    tracks = []
    for idx, track in enumerate(top_tracks_response['items']):
        tracks.append({
            'rank': idx + 1,
            'time_range': time_range,
            'track_id': track['id'],
            'track_name': track['name'],
            'artist_id': track['artists'][0]['id'],
            'artist_name': track['artists'][0]['name'],
            'album_name': track['album']['name'],
            'album_type': track['album']['album_type'],
            'release_date': track['album']['release_date'],
            'popularity': track['popularity'],
            'duration_ms': track['duration_ms'],
            'explicit': track['explicit'],
        })

    df = pd.DataFrame(tracks)
    df['duration_seconds'] = df['duration_ms'] / 1000
    df['release_year'] = df['release_date'].str[:4].astype(int)

    return df


def process_top_artists(top_artists_response, time_range):
    """Convert top artists API response to DataFrame"""
    artists = []
    for idx, artist in enumerate(top_artists_response['items']):
        artists.append({
            'rank': idx + 1,
            'time_range': time_range,
            'artist_id': artist['id'],
            'artist_name': artist['name'],
            'genres': ','.join(artist['genres']) if artist['genres'] else '',
            'genre_list': artist['genres'],
            'popularity': artist['popularity'],
            'followers': artist['followers']['total'],
        })

    return pd.DataFrame(artists)


def compute_snapshot_metrics(recent_df, tracks_s, tracks_m, tracks_l, artists_s, artists_m, artists_l):
    """
    Compute derived metrics from snapshot data
    Returns dict suitable for JSON serialization
    """

    # Combine all top tracks for overall stats
    all_top_tracks = pd.concat([tracks_s, tracks_m, tracks_l]).drop_duplicates(subset=['track_id'])
    all_top_artists = pd.concat([artists_s, artists_m, artists_l]).drop_duplicates(subset=['artist_id'])

    metrics = {
        'recent_listening': {
            'total_tracks': len(recent_df),
            'unique_tracks': recent_df['track_id'].nunique(),
            'unique_artists': recent_df['artist_id'].nunique(),
            'avg_popularity': float(recent_df['popularity'].mean()),
            'explicit_ratio': float(recent_df['explicit'].mean()),
            'avg_duration_minutes': float(recent_df['duration_seconds'].mean() / 60),
            'avg_release_year': float(recent_df['release_year'].mean()),
        },
        'top_tracks': {
            'short_term_avg_popularity': float(tracks_s['popularity'].mean()),
            'medium_term_avg_popularity': float(tracks_m['popularity'].mean()),
            'long_term_avg_popularity': float(tracks_l['popularity'].mean()),
            'short_explicit_ratio': float(tracks_s['explicit'].mean()),
            'medium_explicit_ratio': float(tracks_m['explicit'].mean()),
            'long_explicit_ratio': float(tracks_l['explicit'].mean()),
        },
        'top_artists': {
            'short_term_avg_popularity': float(artists_s['popularity'].mean()),
            'medium_term_avg_popularity': float(artists_m['popularity'].mean()),
            'long_term_avg_popularity': float(artists_l['popularity'].mean()),
            'short_term_avg_followers': float(artists_s['followers'].mean()),
            'medium_term_avg_followers': float(artists_m['followers'].mean()),
            'long_term_avg_followers': float(artists_l['followers'].mean()),
        },
        'diversity': {
            'artist_diversity': float(recent_df['artist_id'].nunique() / len(recent_df)),
            'mainstream_score': float(recent_df['popularity'].mean()),
            'unique_genres': len(set([g for genres in all_top_artists['genre_list'] for g in genres])),
        },
        'taste_consistency': {
            # Overlap between short-term and long-term top tracks
            'short_vs_long_overlap': len(set(tracks_s['track_id']) & set(tracks_l['track_id'])),
            'short_vs_long_overlap_pct': float(len(set(tracks_s['track_id']) & set(tracks_l['track_id'])) / 50 * 100),
        }
    }

    return metrics
```

---

### Step 2: Update `0_Data_Sync.py`

```python
# app/pages/0_Data_Sync.py

import streamlit as st
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from func.ui_components import apply_page_config, get_custom_css
from func.page_auth import require_auth
from func.data_collection import should_refresh_data, collect_comprehensive_snapshot

apply_page_config()
st.markdown(get_custom_css(), unsafe_allow_html=True)

sp, profile = require_auth()
if not sp:
    st.warning("Please connect your Spotify account to continue.")
    st.stop()

user_id = profile['id']

st.title("ðŸ“Š Data Sync")

# Check if refresh needed
force_sync = st.session_state.get('force_sync', False)
refresh_needed = force_sync or should_refresh_data(user_id)

if not refresh_needed:
    st.success("âœ… Your data is up to date!")
    st.info("We automatically refresh your data every 24 hours to keep your insights current.")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("Force Refresh Now", type="secondary"):
            st.session_state['force_sync'] = True
            st.rerun()
    with col2:
        if st.button("Go to Dashboard", type="primary"):
            st.switch_page("pages/1_Dashboard.py")
    st.stop()

# Sync needed - show progress
st.info("â³ Collecting your listening data from Spotify...")
st.caption("This will take about 60-90 seconds. We're fetching:")
st.caption("â€¢ Your top 50 tracks (last 4 weeks, 6 months, and all-time)")
st.caption("â€¢ Your top 50 artists (last 4 weeks, 6 months, and all-time)")
st.caption("â€¢ Your 50 most recently played tracks")

# Progress tracking
progress_bar = st.progress(0)
status_text = st.empty()

steps = [
    ("Connecting to Spotify API...", 0.05),
    ("Fetching user profile...", 0.15),
    ("Fetching recently played tracks...", 0.25),
    ("Fetching top tracks (last 4 weeks)...", 0.35),
    ("Fetching top tracks (last 6 months)...", 0.45),
    ("Fetching top tracks (all-time)...", 0.55),
    ("Fetching top artists (last 4 weeks)...", 0.65),
    ("Fetching top artists (last 6 months)...", 0.75),
    ("Fetching top artists (all-time)...", 0.85),
    ("Computing analytics metrics...", 0.92),
    ("Saving to cloud storage...", 0.98),
    ("Finalizing...", 1.0)
]

for step_text, progress in steps:
    status_text.text(step_text)
    progress_bar.progress(progress)
    time.sleep(0.3)

# Actually collect data
success, snapshot_timestamp = collect_comprehensive_snapshot(sp, user_id, force=force_sync)

if success:
    progress_bar.progress(1.0)
    status_text.text("âœ… Data sync complete!")
    st.success("ðŸŽ‰ All set! Your personalized insights are ready.")

    # Clear force sync flag
    if 'force_sync' in st.session_state:
        del st.session_state['force_sync']

    st.balloons()
    time.sleep(2)
    st.switch_page("pages/1_Dashboard.py")
else:
    st.error("âŒ Data sync failed. Please try again or check your connection.")
    if st.button("Retry Sync"):
        st.rerun()
```

---

### Step 3: Load Current Data in Dashboards

```python
# Example: app/pages/1_Dashboard.py

import streamlit as st
import pandas as pd
from func.page_auth import require_auth
from func.s3_storage import load_parquet_from_r2, load_from_r2

sp, profile = require_auth()
if not sp:
    st.stop()

user_id = profile['id']

# Load current snapshot (cached for 1 hour)
@st.cache_data(ttl=3600)
def load_current_data(user_id):
    """Load current snapshot for dashboard"""
    try:
        recent_tracks = load_parquet_from_r2(f'users/{user_id}/current/recent_tracks.parquet')
        top_tracks_short = load_parquet_from_r2(f'users/{user_id}/current/top_tracks_short.parquet')
        top_tracks_medium = load_parquet_from_r2(f'users/{user_id}/current/top_tracks_medium.parquet')
        top_tracks_long = load_parquet_from_r2(f'users/{user_id}/current/top_tracks_long.parquet')
        top_artists_short = load_parquet_from_r2(f'users/{user_id}/current/top_artists_short.parquet')
        top_artists_medium = load_parquet_from_r2(f'users/{user_id}/current/top_artists_medium.parquet')
        top_artists_long = load_parquet_from_r2(f'users/{user_id}/current/top_artists_long.parquet')
        metrics = load_from_r2(f'users/{user_id}/current/computed_metrics.json')

        return {
            'recent_tracks': recent_tracks,
            'top_tracks': {
                'short': top_tracks_short,
                'medium': top_tracks_medium,
                'long': top_tracks_long
            },
            'top_artists': {
                'short': top_artists_short,
                'medium': top_artists_medium,
                'long': top_artists_long
            },
            'metrics': metrics
        }
    except Exception as e:
        st.error(f"Failed to load data: {e}")
        st.info("Please complete Data Sync first.")
        if st.button("Go to Data Sync"):
            st.switch_page("pages/0_Data_Sync.py")
        st.stop()

# Load data
data = load_current_data(user_id)

# Now build dashboard using data['recent_tracks'], data['top_tracks']['short'], etc.
st.header("ðŸŽµ Your Music Dashboard")

# Example: Top Artists
st.subheader("Top Artists This Month")
top_5 = data['top_artists']['short'].head(5)
st.dataframe(top_5[['rank', 'artist_name', 'popularity', 'genres']])

# Example: Taste Consistency
overlap_pct = data['metrics']['taste_consistency']['short_vs_long_overlap_pct']
if overlap_pct > 60:
    st.success(f"Musical Consistency: {overlap_pct:.0f}% of your current favorites are all-time classics!")
else:
    st.info(f"Musical Explorer: Only {overlap_pct:.0f}% overlap - you're always discovering new music!")
```

---

### Step 4: Deep User Analytics (Special Handling)

```python
# app/pages/6_Deep_User.py

import streamlit as st
from func.page_auth import require_auth
from func.s3_storage import list_r2_objects, load_parquet_from_r2

sp, profile = require_auth()
if not sp:
    st.stop()

user_id = profile['id']

st.header("ðŸ“Š Deep User Analytics")
st.caption("Track your listening evolution over time")

# Count snapshots
@st.cache_data(ttl=3600)
def count_snapshots(user_id):
    """Count how many historical snapshots exist"""
    try:
        snapshots = list_r2_objects(f'users/{user_id}/snapshots/')
        # Each snapshot is a directory, count unique timestamps
        unique_snapshots = set([s.split('/')[3] for s in snapshots if len(s.split('/')) > 3])
        return len(unique_snapshots)
    except:
        return 0

snapshot_count = count_snapshots(user_id)

if snapshot_count < 2:
    # First-time user experience
    st.info(f"ðŸ“… **Current Status**: {snapshot_count} snapshot collected")
    st.warning("Deep User Analytics requires multiple snapshots to show trends over time.")

    st.markdown("""
    ### What you'll see with more data:

    - ðŸ“ˆ **Artist Evolution**: How your top artists change week-over-week
    - ðŸ• **Listening Patterns**: Temporal shifts in your music habits
    - ðŸŽ¯ **Taste Trajectory**: Are you becoming more mainstream or niche?
    - ðŸŒ **Genre Drift**: How your genre preferences evolve
    - ðŸ” **Discovery Trends**: Your exploration rate over time

    ### How it works:

    We automatically collect a snapshot every 24 hours when you visit the dashboard.
    Come back in a few days to see your musical journey unfold!
    """)

    # Show single snapshot preview
    st.subheader("Current Snapshot Preview")

    current_data = load_current_data(user_id)

    # Show current top artists
    st.write("**Top Artists Right Now:**")
    st.dataframe(current_data['top_artists']['short'].head(10)[['rank', 'artist_name', 'popularity']])

    # Placeholder charts
    st.info("Charts will appear here once you have multiple snapshots to compare.")

else:
    # Multi-snapshot experience
    st.success(f"âœ… {snapshot_count} snapshots collected - showing historical analysis")

    # Load and compare snapshots
    # Implementation here...
    st.write("Historical analysis goes here")
```

---

## Rate Limiting & Performance

### API Call Budget

**Initial Sync**: 8 API calls
- 1 user profile
- 1 recent tracks
- 3 top tracks (short/medium/long)
- 3 top artists (short/medium/long)

**Rate Limit**: ~180 calls/minute
**Usage**: 8/180 = 4.4% of rate limit
**Safe**: âœ… Well within limits

### Timing Estimate

| Step | Time (seconds) |
|------|----------------|
| API calls (8 total) | 10-15 |
| DataFrame processing | 5-10 |
| R2 upload (8 parquet + 2 JSON) | 10-20 |
| UI rendering | 5-10 |
| **Total** | **30-55 seconds** |

**Target**: 90 seconds
**Actual**: 30-55 seconds
**Margin**: âœ… Comfortable buffer

### Optimization Strategies

1. **Parallel API Calls**:
```python
from concurrent.futures import ThreadPoolExecutor

def fetch_all_data_parallel(sp):
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            'recent': executor.submit(sp.current_user_recently_played, limit=50),
            'tracks_short': executor.submit(sp.current_user_top_tracks, time_range='short_term', limit=50),
            # ... etc
        }
        results = {key: future.result() for key, future in futures.items()}
    return results
```

2. **Streamlit Caching**:
```python
@st.cache_data(ttl=3600)
def load_current_data(user_id):
    # Cached for 1 hour, instant loads
    pass
```

3. **Lazy Loading**:
```python
# Only load Deep User snapshots when page is accessed
# Don't load all snapshots on every dashboard page
```

---

## Summary

### Architecture Comparison

| Aspect | Old (Multi-Snapshot Focus) | New (First-Time Optimized) |
|--------|---------------------------|----------------------------|
| Primary UX | Returning user | First-time user |
| Dashboard data | Aggregated snapshots | Current snapshot |
| Deep User | Core feature | Bonus feature |
| Sync frequency | Every session | Once per 24hrs |
| File structure | Snapshots/ only | current/ + snapshots/ |
| Loading time | Variable (aggregation) | Fast (single directory) |
| First visit value | Limited | Complete |

### Key Decisions Made

1. âœ… **File Structure**: `current/` + `snapshots/` (simple, clear separation)
2. âœ… **Initial Sync**: 8 API calls, <60 seconds, all data needed
3. âœ… **Dashboard Strategy**: All pages use `current/` except Deep User
4. âœ… **Deep User UX**: Show single point + "come back" message for first-timers
5. âœ… **Refresh Strategy**: Auto-refresh if >24hrs, manual override available
6. âœ… **Skip Playlists**: Future feature, keeps initial sync under 90 seconds

### Next Steps

Ready to implement this architecture! The plan addresses:

- âœ… First-time user gets instant value (all dashboards work)
- âœ… Simple file structure (current/ used by all dashboards)
- âœ… Deep User as optional power feature (requires multiple visits)
- âœ… Fast loading (<60 seconds sync, instant dashboard loads)
- âœ… Smart refresh (24hr auto-refresh, manual override)
- âœ… No rate limiting concerns (8 calls well within 180/min limit)

Want me to implement this in your codebase?
